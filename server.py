#!/usr/bin/env python3
"""Fix-Pic åç«¯æœåŠ¡ - æŠ å›¾æ¢èƒŒæ™¯"""

import io
import os
import base64
import numpy as np
from flask import Flask, request, jsonify
from flask_cors import CORS
from PIL import Image
from rembg import remove

# SAM ç›¸å…³
import torch
from segment_anything import sam_model_registry, SamPredictor

# è¯­ä¹‰åˆ†å‰²ç›¸å…³
from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation
import torch.nn.functional as F

app = Flask(__name__)
CORS(app)

# SAM æ¨¡å‹è·¯å¾„
SAM_CHECKPOINT = os.path.join(os.path.dirname(__file__), 'models', 'sam_vit_b.pth')
SAM_MODEL_TYPE = 'vit_b'

# æœè£…åˆ†å‰²ç±»åˆ«ï¼ˆå¯¹åº” mattmdjaga/segformer_b2_clothes æ¨¡å‹ï¼‰
CLOTHES_LABELS = {
    0: 'Background',
    1: 'Hat',
    2: 'Hair',
    3: 'Sunglasses',
    4: 'Upper-clothes',
    5: 'Skirt',
    6: 'Pants',
    7: 'Dress',
    8: 'Belt',
    9: 'Left-shoe',
    10: 'Right-shoe',
    11: 'Face',
    12: 'Left-leg',
    13: 'Right-leg',
    14: 'Left-arm',
    15: 'Right-arm',
    16: 'Bag',
    17: 'Scarf'
}

# ä¸­æ–‡æ ‡ç­¾æ˜ å°„
CLOTHES_LABELS_CN = {
    0: 'èƒŒæ™¯',
    1: 'å¸½å­',
    2: 'å¤´å‘',
    3: 'å¤ªé˜³é•œ',
    4: 'ä¸Šè¡£',
    5: 'è£™å­',
    6: 'è£¤å­',
    7: 'è¿è¡£è£™',
    8: 'è…°å¸¦',
    9: 'å·¦é‹',
    10: 'å³é‹',
    11: 'è„¸éƒ¨',
    12: 'å·¦è…¿',
    13: 'å³è…¿',
    14: 'å·¦è‡‚',
    15: 'å³è‡‚',
    16: 'åŒ…',
    17: 'å›´å·¾'
}

# å»¶è¿ŸåŠ è½½ SAM
sam_predictor = None

# å»¶è¿ŸåŠ è½½æœè£…åˆ†å‰²æ¨¡å‹
clothes_processor = None
clothes_model = None

def get_sam_predictor():
    """å»¶è¿ŸåŠ è½½ SAM æ¨¡å‹"""
    global sam_predictor
    if sam_predictor is None:
        print("ğŸ”„ æ­£åœ¨åŠ è½½ SAM æ¨¡å‹...")
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
        sam.to(device)
        sam_predictor = SamPredictor(sam)
        print(f"âœ… SAM æ¨¡å‹åŠ è½½å®Œæˆ (è®¾å¤‡: {device})")
    return sam_predictor

def get_clothes_model():
    """å»¶è¿ŸåŠ è½½æœè£…åˆ†å‰²æ¨¡å‹"""
    global clothes_processor, clothes_model
    if clothes_model is None:
        print("ğŸ”„ æ­£åœ¨åŠ è½½æœè£…åˆ†å‰²æ¨¡å‹...")
        clothes_processor = SegformerImageProcessor.from_pretrained("mattmdjaga/segformer_b2_clothes")
        clothes_model = AutoModelForSemanticSegmentation.from_pretrained("mattmdjaga/segformer_b2_clothes")
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        clothes_model.to(device)
        print(f"âœ… æœè£…åˆ†å‰²æ¨¡å‹åŠ è½½å®Œæˆ (è®¾å¤‡: {device})")
    return clothes_processor, clothes_model

@app.route('/api/remove-bg', methods=['POST'])
def remove_background():
    """æŠ å›¾ - å»é™¤èƒŒæ™¯"""
    try:
        if 'image' not in request.files:
            return jsonify({'error': 'è¯·ä¸Šä¼ å›¾ç‰‡'}), 400

        file = request.files['image']
        input_image = Image.open(file.stream)

        # ä½¿ç”¨ rembg å»é™¤èƒŒæ™¯
        output_image = remove(input_image)

        # è½¬æ¢ä¸º base64
        buffered = io.BytesIO()
        output_image.save(buffered, format='PNG')
        img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')

        return jsonify({
            'success': True,
            'image': f'data:image/png;base64,{img_base64}',
            'width': output_image.width,
            'height': output_image.height
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/change-bg', methods=['POST'])
def change_background():
    """æ¢èƒŒæ™¯"""
    try:
        if 'image' not in request.files:
            return jsonify({'error': 'è¯·ä¸Šä¼ å›¾ç‰‡'}), 400

        file = request.files['image']
        input_image = Image.open(file.stream)

        # å»é™¤èƒŒæ™¯
        fg_image = remove(input_image)

        # è·å–æ–°èƒŒæ™¯
        if 'background' in request.files:
            # ç”¨æˆ·ä¸Šä¼ çš„èƒŒæ™¯å›¾
            bg_file = request.files['background']
            bg_image = Image.open(bg_file.stream).convert('RGBA')
            bg_image = bg_image.resize(fg_image.size, Image.Resampling.LANCZOS)
        elif 'bg_color' in request.form:
            # çº¯è‰²èƒŒæ™¯
            color = request.form['bg_color']
            if color.startswith('#'):
                r = int(color[1:3], 16)
                g = int(color[3:5], 16)
                b = int(color[5:7], 16)
            else:
                r, g, b = 255, 255, 255
            bg_image = Image.new('RGBA', fg_image.size, (r, g, b, 255))
        else:
            # é»˜è®¤ç™½è‰²èƒŒæ™¯
            bg_image = Image.new('RGBA', fg_image.size, (255, 255, 255, 255))

        # åˆæˆ
        result = Image.alpha_composite(bg_image, fg_image)

        # è½¬æ¢ä¸º base64
        buffered = io.BytesIO()
        result.save(buffered, format='PNG')
        img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')

        return jsonify({
            'success': True,
            'image': f'data:image/png;base64,{img_base64}',
            'width': result.width,
            'height': result.height
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/sam-segment', methods=['POST'])
def sam_segment():
    """SAM ç‚¹å‡»åˆ†å‰²"""
    try:
        if 'image' not in request.files:
            return jsonify({'error': 'è¯·ä¸Šä¼ å›¾ç‰‡'}), 400

        # è·å–ç‚¹å‡»åæ ‡
        points_json = request.form.get('points', '[]')
        import json
        points = json.loads(points_json)

        if not points:
            return jsonify({'error': 'è¯·ç‚¹å‡»é€‰æ‹©è¦æŠ å‡ºçš„åŒºåŸŸ'}), 400

        file = request.files['image']
        input_image = Image.open(file.stream).convert('RGB')
        image_array = np.array(input_image)

        # è·å– SAM é¢„æµ‹å™¨
        predictor = get_sam_predictor()
        predictor.set_image(image_array)

        # å‡†å¤‡ç‚¹å‡»ç‚¹å’Œæ ‡ç­¾
        input_points = np.array([[p['x'], p['y']] for p in points])
        input_labels = np.array([p.get('label', 1) for p in points])  # 1=å‰æ™¯, 0=èƒŒæ™¯

        # é¢„æµ‹åˆ†å‰²æ©ç 
        masks, scores, _ = predictor.predict(
            point_coords=input_points,
            point_labels=input_labels,
            multimask_output=True
        )

        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„æ©ç 
        best_idx = np.argmax(scores)
        mask = masks[best_idx]

        # åº”ç”¨æ©ç åˆ›å»ºé€æ˜å›¾ (ä½¿ç”¨ numpy åŠ é€Ÿ)
        input_rgba = np.array(input_image.convert('RGBA'))
        output_array = np.zeros_like(input_rgba)
        output_array[mask] = input_rgba[mask]
        output_image = Image.fromarray(output_array, 'RGBA')

        # è½¬æ¢ä¸º base64
        buffered = io.BytesIO()
        output_image.save(buffered, format='PNG')
        img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')

        return jsonify({
            'success': True,
            'image': f'data:image/png;base64,{img_base64}',
            'width': output_image.width,
            'height': output_image.height,
            'score': float(scores[best_idx])
        })
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500


@app.route('/api/clothes-parse', methods=['POST'])
def clothes_parse():
    """æœè£…è§£æ - è¿”å›æ£€æµ‹åˆ°çš„ç±»åˆ«"""
    try:
        if 'image' not in request.files:
            return jsonify({'error': 'è¯·ä¸Šä¼ å›¾ç‰‡'}), 400

        file = request.files['image']
        input_image = Image.open(file.stream).convert('RGB')

        # è·å–æ¨¡å‹
        processor, model = get_clothes_model()
        device = next(model.parameters()).device

        # é¢„å¤„ç†
        inputs = processor(images=input_image, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # æ¨ç†
        with torch.no_grad():
            outputs = model(**inputs)

        # åå¤„ç†
        logits = outputs.logits
        upsampled_logits = F.interpolate(
            logits,
            size=input_image.size[::-1],
            mode='bilinear',
            align_corners=False
        )
        pred_seg = upsampled_logits.argmax(dim=1)[0].cpu().numpy()

        # ç»Ÿè®¡æ£€æµ‹åˆ°çš„ç±»åˆ«
        unique_labels = np.unique(pred_seg)
        detected = []
        for label_id in unique_labels:
            if label_id == 0:  # è·³è¿‡èƒŒæ™¯
                continue
            pixel_count = np.sum(pred_seg == label_id)
            if pixel_count > 100:  # è¿‡æ»¤å¤ªå°çš„åŒºåŸŸ
                detected.append({
                    'id': int(label_id),
                    'name': CLOTHES_LABELS.get(label_id, 'Unknown'),
                    'name_cn': CLOTHES_LABELS_CN.get(label_id, 'æœªçŸ¥'),
                    'pixels': int(pixel_count)
                })

        # æŒ‰åƒç´ æ•°æ’åº
        detected.sort(key=lambda x: x['pixels'], reverse=True)

        return jsonify({
            'success': True,
            'categories': detected,
            'width': input_image.width,
            'height': input_image.height
        })
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500


@app.route('/api/clothes-segment', methods=['POST'])
def clothes_segment():
    """æœè£…åˆ†å‰² - æ ¹æ®é€‰æ‹©çš„ç±»åˆ«æŠ å›¾"""
    try:
        if 'image' not in request.files:
            return jsonify({'error': 'è¯·ä¸Šä¼ å›¾ç‰‡'}), 400

        # è·å–è¦åˆ†å‰²çš„ç±»åˆ«IDåˆ—è¡¨
        import json
        categories_json = request.form.get('categories', '[]')
        selected_categories = json.loads(categories_json)

        if not selected_categories:
            return jsonify({'error': 'è¯·é€‰æ‹©è¦æŠ å‡ºçš„ç±»åˆ«'}), 400

        file = request.files['image']
        input_image = Image.open(file.stream).convert('RGB')

        # è·å–æ¨¡å‹
        processor, model = get_clothes_model()
        device = next(model.parameters()).device

        # é¢„å¤„ç†
        inputs = processor(images=input_image, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # æ¨ç†
        with torch.no_grad():
            outputs = model(**inputs)

        # åå¤„ç†
        logits = outputs.logits
        upsampled_logits = F.interpolate(
            logits,
            size=input_image.size[::-1],
            mode='bilinear',
            align_corners=False
        )
        pred_seg = upsampled_logits.argmax(dim=1)[0].cpu().numpy()

        # åˆ›å»ºæ©ç ï¼ˆé€‰ä¸­ç±»åˆ«çš„å¹¶é›†ï¼‰
        mask = np.zeros(pred_seg.shape, dtype=bool)
        for cat_id in selected_categories:
            mask |= (pred_seg == cat_id)

        # åº”ç”¨æ©ç åˆ›å»ºé€æ˜å›¾
        input_rgba = np.array(input_image.convert('RGBA'))
        output_array = np.zeros_like(input_rgba)
        output_array[mask] = input_rgba[mask]
        output_image = Image.fromarray(output_array, 'RGBA')

        # è½¬æ¢ä¸º base64
        buffered = io.BytesIO()
        output_image.save(buffered, format='PNG')
        img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')

        return jsonify({
            'success': True,
            'image': f'data:image/png;base64,{img_base64}',
            'width': output_image.width,
            'height': output_image.height
        })
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500


@app.route('/health', methods=['GET'])
def health():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({'status': 'ok'})


if __name__ == '__main__':
    print("ğŸš€ Fix-Pic åç«¯æœåŠ¡å¯åŠ¨ä¸­...")
    print("ğŸ“ http://localhost:5001")
    print("ğŸ’¡ é¦–æ¬¡æŠ å›¾ä¼šä¸‹è½½æ¨¡å‹ï¼ˆçº¦170MBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…")
    app.run(host='0.0.0.0', port=5001, debug=True)
